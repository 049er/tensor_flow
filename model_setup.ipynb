{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(1)\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2632, 8)\n",
      "(2632, 3)\n"
     ]
    }
   ],
   "source": [
    "#Data\n",
    "corner_points = np.empty((0,8), float)\n",
    "answers = np.empty((0,3), float)\n",
    "\n",
    "#0,0,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/training/n/x0_y0_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "corner_points = np.append(corner_points, data, axis=0)\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    answers = np.vstack([answers, [0,0,0]])\n",
    "\n",
    "#30,0,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/training/x/x30_y0_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "corner_points =  np.append(corner_points, data, axis=0) \n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    answers = np.vstack([answers, [30,0,0]])\n",
    "\n",
    "#45,0,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/training/x/x45_y0_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "corner_points =  np.append(corner_points, data, axis=0) \n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    answers = np.vstack([answers, [45,0,0]])   \n",
    "   \n",
    "#50,0,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/training/x/x50_y0_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "corner_points =  np.append(corner_points, data, axis=0) \n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    answers = np.vstack([answers, [50,0,0]])   \n",
    "   \n",
    "#60,0,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/training/x/x60_y0_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "corner_points =  np.append(corner_points, data, axis=0) \n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    answers = np.vstack([answers, [60,0,0]])\n",
    "    \n",
    "    \n",
    "#0,30,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/training/y/x0_y30_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "corner_points = np.append(corner_points, data, axis=0)\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    answers = np.vstack([answers, [0,30,0]])\n",
    "    \n",
    "#0,45,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/training/y/x0_y45_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "corner_points = np.append(corner_points, data, axis=0)\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    answers = np.vstack([answers, [0,45,0]])\n",
    "    \n",
    "#0,60,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/training/y/x0_y60_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "corner_points = np.append(corner_points, data, axis=0)\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    answers = np.vstack([answers, [0,60,0]])\n",
    "\n",
    "#0,-30,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/training/y/x0_y_neg_30_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "corner_points = np.append(corner_points, data, axis=0)\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    answers = np.vstack([answers, [0,-30,0]])\n",
    "\n",
    "\n",
    "#0,-60,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/training/y/x0_y_neg_60_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "corner_points = np.append(corner_points, data, axis=0)\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    answers = np.vstack([answers, [0,-60,0]])\n",
    "\n",
    "\n",
    "    \n",
    "print(corner_points.shape)\n",
    "print(answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 15:09:53.359266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 15:09:53.437697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 15:09:53.437949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 15:09:53.443465: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-14 15:09:53.448901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 15:09:53.449261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 15:09:53.449506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 15:09:54.667009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 15:09:54.667199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 15:09:54.667329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 15:09:54.667448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5071 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "83/83 [==============================] - 2s 2ms/step - loss: 392.3615\n",
      "Epoch 2/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 218.4746\n",
      "Epoch 3/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 155.0949\n",
      "Epoch 4/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 122.7560\n",
      "Epoch 5/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 109.2624\n",
      "Epoch 6/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 99.0155\n",
      "Epoch 7/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 95.3624\n",
      "Epoch 8/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 91.2426\n",
      "Epoch 9/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 88.4305\n",
      "Epoch 10/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 83.4610\n",
      "Epoch 11/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 83.7162\n",
      "Epoch 12/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 80.0446\n",
      "Epoch 13/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 77.1030\n",
      "Epoch 14/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 75.0794\n",
      "Epoch 15/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 72.2718\n",
      "Epoch 16/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 70.9911\n",
      "Epoch 17/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 68.7521\n",
      "Epoch 18/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 65.4632\n",
      "Epoch 19/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 65.4693\n",
      "Epoch 20/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 62.5404\n",
      "Epoch 21/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 60.9528\n",
      "Epoch 22/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 57.7053\n",
      "Epoch 23/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 57.0512\n",
      "Epoch 24/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 55.5879\n",
      "Epoch 25/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 53.8200\n",
      "Epoch 26/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 52.4132\n",
      "Epoch 27/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 51.1495\n",
      "Epoch 28/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 49.8418\n",
      "Epoch 29/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 50.8855\n",
      "Epoch 30/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 47.7445\n",
      "Epoch 31/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 46.3331\n",
      "Epoch 32/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 44.3380\n",
      "Epoch 33/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 43.3940\n",
      "Epoch 34/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 45.0130\n",
      "Epoch 35/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 42.0564\n",
      "Epoch 36/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 41.8368\n",
      "Epoch 37/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 40.6046\n",
      "Epoch 38/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 38.5936\n",
      "Epoch 39/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 37.2758\n",
      "Epoch 40/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 35.8889\n",
      "Epoch 41/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 34.9189\n",
      "Epoch 42/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 33.9812\n",
      "Epoch 43/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 32.9315\n",
      "Epoch 44/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 32.1313\n",
      "Epoch 45/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 36.3561\n",
      "Epoch 46/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 30.2517\n",
      "Epoch 47/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 29.7396\n",
      "Epoch 48/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 28.7432\n",
      "Epoch 49/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 28.3117\n",
      "Epoch 50/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 29.9864\n",
      "Epoch 51/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 27.6981\n",
      "Epoch 52/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 25.5928\n",
      "Epoch 53/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 24.5713\n",
      "Epoch 54/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 24.2307\n",
      "Epoch 55/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 23.6572\n",
      "Epoch 56/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 22.9382\n",
      "Epoch 57/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 21.4024\n",
      "Epoch 58/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 22.3091\n",
      "Epoch 59/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 21.8002\n",
      "Epoch 60/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 21.3832\n",
      "Epoch 61/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 21.6558\n",
      "Epoch 62/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 19.1377\n",
      "Epoch 63/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 17.9030\n",
      "Epoch 64/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 17.1951\n",
      "Epoch 65/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 17.2386\n",
      "Epoch 66/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 16.5078\n",
      "Epoch 67/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 15.5937\n",
      "Epoch 68/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 15.6119\n",
      "Epoch 69/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 16.1161\n",
      "Epoch 70/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 14.9855\n",
      "Epoch 71/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 18.1341\n",
      "Epoch 72/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 17.1378\n",
      "Epoch 73/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 12.6163\n",
      "Epoch 74/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 13.4807\n",
      "Epoch 75/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 13.0536\n",
      "Epoch 76/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 12.8976\n",
      "Epoch 77/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 13.5873\n",
      "Epoch 78/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 11.6858\n",
      "Epoch 79/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 10.8799\n",
      "Epoch 80/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 11.1992\n",
      "Epoch 81/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 10.9003\n",
      "Epoch 82/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 10.5666\n",
      "Epoch 83/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 10.9731\n",
      "Epoch 84/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 9.8402\n",
      "Epoch 85/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 10.3514\n",
      "Epoch 86/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 8.7686\n",
      "Epoch 87/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 8.9999\n",
      "Epoch 88/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 9.8901\n",
      "Epoch 89/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 8.2929\n",
      "Epoch 90/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 8.0154\n",
      "Epoch 91/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 9.4165\n",
      "Epoch 92/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 8.1007\n",
      "Epoch 93/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 7.9123\n",
      "Epoch 94/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 7.9127\n",
      "Epoch 95/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 6.9634\n",
      "Epoch 96/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 7.6633\n",
      "Epoch 97/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 7.5454\n",
      "Epoch 98/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 7.7296\n",
      "Epoch 99/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 6.6240\n",
      "Epoch 100/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 7.0749\n",
      "Epoch 101/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 8.5922\n",
      "Epoch 102/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 6.9598\n",
      "Epoch 103/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 6.4751\n",
      "Epoch 104/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 6.4795\n",
      "Epoch 105/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 6.1665\n",
      "Epoch 106/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.7130\n",
      "Epoch 107/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.9954\n",
      "Epoch 108/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 6.2086\n",
      "Epoch 109/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 8.5562\n",
      "Epoch 110/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.8163\n",
      "Epoch 111/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 7.0450\n",
      "Epoch 112/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.0701\n",
      "Epoch 113/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 6.3155\n",
      "Epoch 114/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.0115\n",
      "Epoch 115/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.0221\n",
      "Epoch 116/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.2850\n",
      "Epoch 117/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 5.4469\n",
      "Epoch 118/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.5544\n",
      "Epoch 119/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.9441\n",
      "Epoch 120/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.5763\n",
      "Epoch 121/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.1888\n",
      "Epoch 122/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.8467\n",
      "Epoch 123/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.9084\n",
      "Epoch 124/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.6144\n",
      "Epoch 125/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.8278\n",
      "Epoch 126/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.2843\n",
      "Epoch 127/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.2235\n",
      "Epoch 128/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.5430\n",
      "Epoch 129/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.9895\n",
      "Epoch 130/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.8168\n",
      "Epoch 131/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.6884\n",
      "Epoch 132/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.4858\n",
      "Epoch 133/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.8564\n",
      "Epoch 134/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 6.9531\n",
      "Epoch 135/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.8126\n",
      "Epoch 136/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 5.5708\n",
      "Epoch 137/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.5112\n",
      "Epoch 138/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.6321\n",
      "Epoch 139/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.1870\n",
      "Epoch 140/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.9254\n",
      "Epoch 141/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.7677\n",
      "Epoch 142/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.5527\n",
      "Epoch 143/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.4359\n",
      "Epoch 144/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.9364\n",
      "Epoch 145/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.6765\n",
      "Epoch 146/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.2538\n",
      "Epoch 147/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.0135\n",
      "Epoch 148/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.5270\n",
      "Epoch 149/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.5156\n",
      "Epoch 150/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.3772\n",
      "Epoch 151/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.7103\n",
      "Epoch 152/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.6023\n",
      "Epoch 153/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.2045\n",
      "Epoch 154/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.2893\n",
      "Epoch 155/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.5531\n",
      "Epoch 156/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.7487\n",
      "Epoch 157/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.9249\n",
      "Epoch 158/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.6628\n",
      "Epoch 159/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.4155\n",
      "Epoch 160/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.8297\n",
      "Epoch 161/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.2905\n",
      "Epoch 162/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.0452\n",
      "Epoch 163/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.7618\n",
      "Epoch 164/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.4817\n",
      "Epoch 165/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.7424\n",
      "Epoch 166/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.4961\n",
      "Epoch 167/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.4631\n",
      "Epoch 168/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 5.4887\n",
      "Epoch 169/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.3740\n",
      "Epoch 170/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.8075\n",
      "Epoch 171/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.4651\n",
      "Epoch 172/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.7145\n",
      "Epoch 173/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.8202\n",
      "Epoch 174/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.2308\n",
      "Epoch 175/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.9375\n",
      "Epoch 176/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.8770\n",
      "Epoch 177/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.9170\n",
      "Epoch 178/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.5269\n",
      "Epoch 179/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.5146\n",
      "Epoch 180/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.9064\n",
      "Epoch 181/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.6019\n",
      "Epoch 182/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.5639\n",
      "Epoch 183/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.8498\n",
      "Epoch 184/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.6779\n",
      "Epoch 185/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.5786\n",
      "Epoch 186/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.5155\n",
      "Epoch 187/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.6872\n",
      "Epoch 188/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1956\n",
      "Epoch 189/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.2951\n",
      "Epoch 190/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.3077\n",
      "Epoch 191/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.4229\n",
      "Epoch 192/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.2391\n",
      "Epoch 193/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.8259\n",
      "Epoch 194/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.2877\n",
      "Epoch 195/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.7142\n",
      "Epoch 196/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9796\n",
      "Epoch 197/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.9110\n",
      "Epoch 198/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.6560\n",
      "Epoch 199/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.0751\n",
      "Epoch 200/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9837\n",
      "Epoch 201/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.3622\n",
      "Epoch 202/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.7145\n",
      "Epoch 203/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.9870\n",
      "Epoch 204/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1273\n",
      "Epoch 205/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 4.1403\n",
      "Epoch 206/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.6102\n",
      "Epoch 207/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9355\n",
      "Epoch 208/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1557\n",
      "Epoch 209/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.5730\n",
      "Epoch 210/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.2447\n",
      "Epoch 211/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1389\n",
      "Epoch 212/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.7483\n",
      "Epoch 213/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.2495\n",
      "Epoch 214/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.9896\n",
      "Epoch 215/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.5386\n",
      "Epoch 216/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.0341\n",
      "Epoch 217/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9923\n",
      "Epoch 218/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9473\n",
      "Epoch 219/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.0738\n",
      "Epoch 220/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7204\n",
      "Epoch 221/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9375\n",
      "Epoch 222/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.4630\n",
      "Epoch 223/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.1922\n",
      "Epoch 224/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.4268\n",
      "Epoch 225/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.5207\n",
      "Epoch 226/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.3386\n",
      "Epoch 227/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8529\n",
      "Epoch 228/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1313\n",
      "Epoch 229/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5577\n",
      "Epoch 230/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9539\n",
      "Epoch 231/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9811\n",
      "Epoch 232/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9420\n",
      "Epoch 233/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8281\n",
      "Epoch 234/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7920\n",
      "Epoch 235/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.0779\n",
      "Epoch 236/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.3113\n",
      "Epoch 237/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8283\n",
      "Epoch 238/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.2538\n",
      "Epoch 239/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.3337\n",
      "Epoch 240/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.0259\n",
      "Epoch 241/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5714\n",
      "Epoch 242/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6969\n",
      "Epoch 243/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8885\n",
      "Epoch 244/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1905\n",
      "Epoch 245/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.3637\n",
      "Epoch 246/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5559\n",
      "Epoch 247/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1860\n",
      "Epoch 248/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1033\n",
      "Epoch 249/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7211\n",
      "Epoch 250/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7746\n",
      "Epoch 251/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.5517\n",
      "Epoch 252/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8356\n",
      "Epoch 253/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9268\n",
      "Epoch 254/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9010\n",
      "Epoch 255/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7423\n",
      "Epoch 256/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0743\n",
      "Epoch 257/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8519\n",
      "Epoch 258/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8617\n",
      "Epoch 259/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1472\n",
      "Epoch 260/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.0260\n",
      "Epoch 261/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9748\n",
      "Epoch 262/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9455\n",
      "Epoch 263/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.0635\n",
      "Epoch 264/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9227\n",
      "Epoch 265/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6816\n",
      "Epoch 266/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7902\n",
      "Epoch 267/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7054\n",
      "Epoch 268/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6579\n",
      "Epoch 269/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6772\n",
      "Epoch 270/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7120\n",
      "Epoch 271/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.2970\n",
      "Epoch 272/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5763\n",
      "Epoch 273/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7448\n",
      "Epoch 274/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8984\n",
      "Epoch 275/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.0125\n",
      "Epoch 276/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5971\n",
      "Epoch 277/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.2081\n",
      "Epoch 278/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4943\n",
      "Epoch 279/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.3103\n",
      "Epoch 280/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6615\n",
      "Epoch 281/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1415\n",
      "Epoch 282/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.2273\n",
      "Epoch 283/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5452\n",
      "Epoch 284/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9633\n",
      "Epoch 285/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7523\n",
      "Epoch 286/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3477\n",
      "Epoch 287/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5583\n",
      "Epoch 288/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4894\n",
      "Epoch 289/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4707\n",
      "Epoch 290/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5024\n",
      "Epoch 291/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9015\n",
      "Epoch 292/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5207\n",
      "Epoch 293/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5234\n",
      "Epoch 294/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7625\n",
      "Epoch 295/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3798\n",
      "Epoch 296/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7532\n",
      "Epoch 297/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4692\n",
      "Epoch 298/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6311\n",
      "Epoch 299/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6459\n",
      "Epoch 300/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8019\n",
      "Epoch 301/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2218\n",
      "Epoch 302/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3060\n",
      "Epoch 303/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6303\n",
      "Epoch 304/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5262\n",
      "Epoch 305/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8039\n",
      "Epoch 306/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3832\n",
      "Epoch 307/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3092\n",
      "Epoch 308/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1848\n",
      "Epoch 309/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0392\n",
      "Epoch 310/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2108\n",
      "Epoch 311/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3571\n",
      "Epoch 312/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4337\n",
      "Epoch 313/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4437\n",
      "Epoch 314/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4485\n",
      "Epoch 315/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3656\n",
      "Epoch 316/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.3383\n",
      "Epoch 317/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6188\n",
      "Epoch 318/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4127\n",
      "Epoch 319/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6113\n",
      "Epoch 320/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4327\n",
      "Epoch 321/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6947\n",
      "Epoch 322/500\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 2.2606\n",
      "Epoch 323/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 4.7895\n",
      "Epoch 324/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5370\n",
      "Epoch 325/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2948\n",
      "Epoch 326/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4450\n",
      "Epoch 327/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9807\n",
      "Epoch 328/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1585\n",
      "Epoch 329/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2965\n",
      "Epoch 330/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1444\n",
      "Epoch 331/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4464\n",
      "Epoch 332/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6290\n",
      "Epoch 333/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2673\n",
      "Epoch 334/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7035\n",
      "Epoch 335/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6315\n",
      "Epoch 336/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9840\n",
      "Epoch 337/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3994\n",
      "Epoch 338/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.7873\n",
      "Epoch 339/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0982\n",
      "Epoch 340/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3929\n",
      "Epoch 341/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9499\n",
      "Epoch 342/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.1331\n",
      "Epoch 343/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6570\n",
      "Epoch 344/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6115\n",
      "Epoch 345/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.1057\n",
      "Epoch 346/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3065\n",
      "Epoch 347/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1768\n",
      "Epoch 348/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3712\n",
      "Epoch 349/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.0412\n",
      "Epoch 350/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5983\n",
      "Epoch 351/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3196\n",
      "Epoch 352/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9449\n",
      "Epoch 353/500\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.3881\n",
      "Epoch 354/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3721\n",
      "Epoch 355/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2281\n",
      "Epoch 356/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4142\n",
      "Epoch 357/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0571\n",
      "Epoch 358/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2383\n",
      "Epoch 359/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5220\n",
      "Epoch 360/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1728\n",
      "Epoch 361/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0892\n",
      "Epoch 362/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2364\n",
      "Epoch 363/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6190\n",
      "Epoch 364/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.2396\n",
      "Epoch 365/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 3.0850\n",
      "Epoch 366/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.5576\n",
      "Epoch 367/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3059\n",
      "Epoch 368/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1156\n",
      "Epoch 369/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2081\n",
      "Epoch 370/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8928\n",
      "Epoch 371/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4425\n",
      "Epoch 372/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2691\n",
      "Epoch 373/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4607\n",
      "Epoch 374/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1231\n",
      "Epoch 375/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9215\n",
      "Epoch 376/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9523\n",
      "Epoch 377/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2999\n",
      "Epoch 378/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3398\n",
      "Epoch 379/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9204\n",
      "Epoch 380/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0245\n",
      "Epoch 381/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2052\n",
      "Epoch 382/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1229\n",
      "Epoch 383/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0190\n",
      "Epoch 384/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7706\n",
      "Epoch 385/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6544\n",
      "Epoch 386/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5485\n",
      "Epoch 387/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4156\n",
      "Epoch 388/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8321\n",
      "Epoch 389/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2836\n",
      "Epoch 390/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7407\n",
      "Epoch 391/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.6057\n",
      "Epoch 392/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.2782\n",
      "Epoch 393/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0053\n",
      "Epoch 394/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9329\n",
      "Epoch 395/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.8003\n",
      "Epoch 396/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9704\n",
      "Epoch 397/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8536\n",
      "Epoch 398/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2871\n",
      "Epoch 399/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3158\n",
      "Epoch 400/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0579\n",
      "Epoch 401/500\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.8636\n",
      "Epoch 402/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2324\n",
      "Epoch 403/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.3673\n",
      "Epoch 404/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9754\n",
      "Epoch 405/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2670\n",
      "Epoch 406/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3346\n",
      "Epoch 407/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8163\n",
      "Epoch 408/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9393\n",
      "Epoch 409/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0421\n",
      "Epoch 410/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2702\n",
      "Epoch 411/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0480\n",
      "Epoch 412/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1320\n",
      "Epoch 413/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6293\n",
      "Epoch 414/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.7522\n",
      "Epoch 415/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9852\n",
      "Epoch 416/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.0976\n",
      "Epoch 417/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9931\n",
      "Epoch 418/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.7694\n",
      "Epoch 419/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0151\n",
      "Epoch 420/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8376\n",
      "Epoch 421/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8968\n",
      "Epoch 422/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4901\n",
      "Epoch 423/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9129\n",
      "Epoch 424/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.2030\n",
      "Epoch 425/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9337\n",
      "Epoch 426/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.1321\n",
      "Epoch 427/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9194\n",
      "Epoch 428/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3421\n",
      "Epoch 429/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9152\n",
      "Epoch 430/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.8936\n",
      "Epoch 431/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.8339\n",
      "Epoch 432/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8934\n",
      "Epoch 433/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9898\n",
      "Epoch 434/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8243\n",
      "Epoch 435/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2017\n",
      "Epoch 436/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0074\n",
      "Epoch 437/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8884\n",
      "Epoch 438/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8637\n",
      "Epoch 439/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1784\n",
      "Epoch 440/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3694\n",
      "Epoch 441/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.0520\n",
      "Epoch 442/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0638\n",
      "Epoch 443/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8422\n",
      "Epoch 444/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0615\n",
      "Epoch 445/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0407\n",
      "Epoch 446/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2781\n",
      "Epoch 447/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0006\n",
      "Epoch 448/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0849\n",
      "Epoch 449/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9240\n",
      "Epoch 450/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9742\n",
      "Epoch 451/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9252\n",
      "Epoch 452/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1591\n",
      "Epoch 453/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9220\n",
      "Epoch 454/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8121\n",
      "Epoch 455/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9096\n",
      "Epoch 456/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0109\n",
      "Epoch 457/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.2811\n",
      "Epoch 458/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.7941\n",
      "Epoch 459/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.7568\n",
      "Epoch 460/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9962\n",
      "Epoch 461/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0801\n",
      "Epoch 462/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0983\n",
      "Epoch 463/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3962\n",
      "Epoch 464/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.7083\n",
      "Epoch 465/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.1198\n",
      "Epoch 466/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7497\n",
      "Epoch 467/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9387\n",
      "Epoch 468/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9768\n",
      "Epoch 469/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.7354\n",
      "Epoch 470/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2059\n",
      "Epoch 471/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.6295\n",
      "Epoch 472/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9452\n",
      "Epoch 473/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6368\n",
      "Epoch 474/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.7222\n",
      "Epoch 475/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.8706\n",
      "Epoch 476/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8291\n",
      "Epoch 477/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.7269\n",
      "Epoch 478/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3444\n",
      "Epoch 479/500\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.9893\n",
      "Epoch 480/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.2651\n",
      "Epoch 481/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8501\n",
      "Epoch 482/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8096\n",
      "Epoch 483/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6957\n",
      "Epoch 484/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5809\n",
      "Epoch 485/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.7034\n",
      "Epoch 486/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.7523\n",
      "Epoch 487/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1396\n",
      "Epoch 488/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0971\n",
      "Epoch 489/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.2641\n",
      "Epoch 490/500\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9523\n",
      "Epoch 491/500\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.8390\n",
      "Epoch 492/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9312\n",
      "Epoch 493/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8701\n",
      "Epoch 494/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.7330\n",
      "Epoch 495/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9254\n",
      "Epoch 496/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 4.0381\n",
      "Epoch 497/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1105\n",
      "Epoch 498/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.8527\n",
      "Epoch 499/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6682\n",
      "Epoch 500/500\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5939\n",
      "Finished training the model\n",
      "0.5938988924026489\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Setup & Training\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(8,)),\n",
    "    tf.keras.layers.Normalization(axis=None),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(units=3,)\n",
    "                             ])\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.0002))\n",
    "\n",
    "history = model.fit(corner_points, answers, epochs=500, verbose=1)\n",
    "\n",
    "print(\"Finished training the model\")\n",
    "# plt.xlabel('Epoch Number')\n",
    "# plt.ylabel(\"Loss Magnitude\")\n",
    "plt.plot(history.history['loss'])\n",
    "print(history.history['loss'][-1])\n",
    "# print(model.predict([0, 0, 1,1,2,2,3,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1065, 8)\n",
      "(1065, 3)\n"
     ]
    }
   ],
   "source": [
    "#Test Data\n",
    "\n",
    "test_cp = np.empty((0,8), float)\n",
    "test_ans = np.empty((0,3), float)\n",
    "\n",
    "#0,0,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/testing/n/test_x0_y0_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "test_cp =  np.append(test_cp, data, axis=0) # append data\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    test_ans = np.vstack([test_ans, [0,0,0]])\n",
    "    \n",
    "#30,0,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/testing/x/test_x30_y0_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "test_cp =  np.append(test_cp, data, axis=0) # append data\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    test_ans = np.vstack([test_ans, [30,0,0]])\n",
    "    \n",
    "#45,0,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/testing/x/test_x45_y0_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "test_cp =  np.append(test_cp, data, axis=0) # append data\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    test_ans = np.vstack([test_ans, [45,0,0]])\n",
    "    \n",
    "#50,0,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/testing/x/test_x50_y0_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "test_cp =  np.append(test_cp, data, axis=0) # append data\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    test_ans = np.vstack([test_ans, [50,0,0]])\n",
    "\n",
    "#60,0,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/testing/x/test_x60_y0_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "test_cp =  np.append(test_cp, data, axis=0) # append data\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    test_ans = np.vstack([test_ans, [60,0,0]])\n",
    "    \n",
    "    \n",
    "#0,30,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/testing/y/test_x0_y30_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "test_cp =  np.append(test_cp, data, axis=0) # append data\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    test_ans = np.vstack([test_ans, [0,30,0]])\n",
    "    \n",
    "#0,45,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/testing/y/test_x0_y45_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "test_cp =  np.append(test_cp, data, axis=0) # append data\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    test_ans = np.vstack([test_ans, [0,45,0]])\n",
    "    \n",
    "# #0,60,0\n",
    "# data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/testing/x/test_x60_y0_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "# test_cp =  np.append(test_cp, data, axis=0) # append data\n",
    "# rows, colms = data.shape # append answers\n",
    "# for x in range(0,rows):\n",
    "#     test_ans = np.vstack([test_ans, [0,60,0]])\n",
    "    \n",
    "#0,-30,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/testing/y/test_x0_y_neg_30_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "test_cp =  np.append(test_cp, data, axis=0) # append data\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    test_ans = np.vstack([test_ans, [0,-30,0]])\n",
    "    \n",
    "#0,-60,0\n",
    "data = np.genfromtxt(\"/home/werner/Projects/tensor_flow/raw_data/testing/y/test_x0_y_neg_60_z0.txt\", dtype=int,encoding=None, delimiter=\",\")\n",
    "test_cp =  np.append(test_cp, data, axis=0) # append data\n",
    "rows, colms = data.shape # append answers\n",
    "for x in range(0,rows):\n",
    "    test_ans = np.vstack([test_ans, [0,-60,0]])\n",
    "    \n",
    "print(test_cp.shape)\n",
    "print(test_ans.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tests:  1065\n",
      "number:  0 / 1065 , % 0.0\n",
      "number:  1 / 1065 , % 0.1\n",
      "number:  2 / 1065 , % 0.2\n",
      "number:  3 / 1065 , % 0.3\n",
      "number:  4 / 1065 , % 0.4\n",
      "number:  5 / 1065 , % 0.5\n",
      "number:  6 / 1065 , % 0.6\n",
      "number:  7 / 1065 , % 0.7\n",
      "number:  8 / 1065 , % 0.8\n",
      "number:  9 / 1065 , % 0.8\n",
      "number:  10 / 1065 , % 0.9\n",
      "number:  11 / 1065 , % 1.0\n",
      "number:  12 / 1065 , % 1.1\n",
      "number:  13 / 1065 , % 1.2\n",
      "number:  14 / 1065 , % 1.3\n",
      "number:  15 / 1065 , % 1.4\n",
      "number:  16 / 1065 , % 1.5\n",
      "number:  17 / 1065 , % 1.6\n",
      "number:  18 / 1065 , % 1.7\n",
      "number:  19 / 1065 , % 1.8\n",
      "number:  20 / 1065 , % 1.9\n",
      "number:  21 / 1065 , % 2.0\n",
      "number:  22 / 1065 , % 2.1\n",
      "number:  23 / 1065 , % 2.2\n",
      "number:  24 / 1065 , % 2.3\n",
      "number:  25 / 1065 , % 2.3\n",
      "number:  26 / 1065 , % 2.4\n",
      "number:  27 / 1065 , % 2.5\n",
      "number:  28 / 1065 , % 2.6\n",
      "number:  29 / 1065 , % 2.7\n",
      "number:  30 / 1065 , % 2.8\n",
      "number:  31 / 1065 , % 2.9\n",
      "number:  32 / 1065 , % 3.0\n",
      "number:  33 / 1065 , % 3.1\n",
      "number:  34 / 1065 , % 3.2\n",
      "number:  35 / 1065 , % 3.3\n",
      "number:  36 / 1065 , % 3.4\n",
      "number:  37 / 1065 , % 3.5\n",
      "number:  38 / 1065 , % 3.6\n",
      "number:  39 / 1065 , % 3.7\n",
      "number:  40 / 1065 , % 3.8\n",
      "number:  41 / 1065 , % 3.8\n",
      "number:  42 / 1065 , % 3.9\n",
      "number:  43 / 1065 , % 4.0\n",
      "number:  44 / 1065 , % 4.1\n",
      "number:  45 / 1065 , % 4.2\n",
      "number:  46 / 1065 , % 4.3\n",
      "number:  47 / 1065 , % 4.4\n",
      "number:  48 / 1065 , % 4.5\n",
      "number:  49 / 1065 , % 4.6\n",
      "number:  50 / 1065 , % 4.7\n",
      "number:  51 / 1065 , % 4.8\n",
      "number:  52 / 1065 , % 4.9\n",
      "number:  53 / 1065 , % 5.0\n",
      "number:  54 / 1065 , % 5.1\n",
      "number:  55 / 1065 , % 5.2\n",
      "number:  56 / 1065 , % 5.3\n",
      "number:  57 / 1065 , % 5.4\n",
      "number:  58 / 1065 , % 5.4\n",
      "number:  59 / 1065 , % 5.5\n",
      "number:  60 / 1065 , % 5.6\n",
      "number:  61 / 1065 , % 5.7\n",
      "number:  62 / 1065 , % 5.8\n",
      "number:  63 / 1065 , % 5.9\n",
      "number:  64 / 1065 , % 6.0\n",
      "number:  65 / 1065 , % 6.1\n",
      "number:  66 / 1065 , % 6.2\n",
      "number:  67 / 1065 , % 6.3\n",
      "number:  68 / 1065 , % 6.4\n",
      "number:  69 / 1065 , % 6.5\n",
      "number:  70 / 1065 , % 6.6\n",
      "number:  71 / 1065 , % 6.7\n",
      "number:  72 / 1065 , % 6.8\n",
      "number:  73 / 1065 , % 6.9\n",
      "number:  74 / 1065 , % 6.9\n",
      "number:  75 / 1065 , % 7.0\n",
      "number:  76 / 1065 , % 7.1\n",
      "number:  77 / 1065 , % 7.2\n",
      "number:  78 / 1065 , % 7.3\n",
      "number:  79 / 1065 , % 7.4\n",
      "number:  80 / 1065 , % 7.5\n",
      "number:  81 / 1065 , % 7.6\n",
      "number:  82 / 1065 , % 7.7\n",
      "number:  83 / 1065 , % 7.8\n",
      "number:  84 / 1065 , % 7.9\n",
      "number:  85 / 1065 , % 8.0\n",
      "number:  86 / 1065 , % 8.1\n",
      "number:  87 / 1065 , % 8.2\n",
      "number:  88 / 1065 , % 8.3\n",
      "number:  89 / 1065 , % 8.4\n",
      "number:  90 / 1065 , % 8.5\n",
      "number:  91 / 1065 , % 8.5\n",
      "number:  92 / 1065 , % 8.6\n",
      "number:  93 / 1065 , % 8.7\n",
      "number:  94 / 1065 , % 8.8\n",
      "number:  95 / 1065 , % 8.9\n",
      "number:  96 / 1065 , % 9.0\n",
      "number:  97 / 1065 , % 9.1\n",
      "number:  98 / 1065 , % 9.2\n",
      "number:  99 / 1065 , % 9.3\n",
      "number:  100 / 1065 , % 9.4\n",
      "number:  101 / 1065 , % 9.5\n",
      "number:  102 / 1065 , % 9.6\n",
      "number:  103 / 1065 , % 9.7\n",
      "number:  104 / 1065 , % 9.8\n",
      "number:  105 / 1065 , % 9.9\n",
      "number:  106 / 1065 , % 10.0\n",
      "number:  107 / 1065 , % 10.0\n",
      "number:  108 / 1065 , % 10.1\n",
      "number:  109 / 1065 , % 10.2\n",
      "number:  110 / 1065 , % 10.3\n",
      "number:  111 / 1065 , % 10.4\n",
      "number:  112 / 1065 , % 10.5\n",
      "number:  113 / 1065 , % 10.6\n",
      "number:  114 / 1065 , % 10.7\n",
      "number:  115 / 1065 , % 10.8\n",
      "number:  116 / 1065 , % 10.9\n",
      "number:  117 / 1065 , % 11.0\n",
      "number:  118 / 1065 , % 11.1\n",
      "number:  119 / 1065 , % 11.2\n",
      "number:  120 / 1065 , % 11.3\n",
      "number:  121 / 1065 , % 11.4\n",
      "number:  122 / 1065 , % 11.5\n",
      "number:  123 / 1065 , % 11.5\n",
      "number:  124 / 1065 , % 11.6\n",
      "number:  125 / 1065 , % 11.7\n",
      "number:  126 / 1065 , % 11.8\n",
      "number:  127 / 1065 , % 11.9\n",
      "number:  128 / 1065 , % 12.0\n",
      "number:  129 / 1065 , % 12.1\n",
      "number:  130 / 1065 , % 12.2\n",
      "number:  131 / 1065 , % 12.3\n",
      "number:  132 / 1065 , % 12.4\n",
      "number:  133 / 1065 , % 12.5\n",
      "number:  134 / 1065 , % 12.6\n",
      "number:  135 / 1065 , % 12.7\n",
      "number:  136 / 1065 , % 12.8\n",
      "number:  137 / 1065 , % 12.9\n",
      "number:  138 / 1065 , % 13.0\n",
      "number:  139 / 1065 , % 13.1\n",
      "number:  140 / 1065 , % 13.1\n",
      "number:  141 / 1065 , % 13.2\n",
      "number:  142 / 1065 , % 13.3\n",
      "number:  143 / 1065 , % 13.4\n",
      "number:  144 / 1065 , % 13.5\n",
      "number:  145 / 1065 , % 13.6\n",
      "number:  146 / 1065 , % 13.7\n",
      "number:  147 / 1065 , % 13.8\n",
      "number:  148 / 1065 , % 13.9\n",
      "number:  149 / 1065 , % 14.0\n",
      "number:  150 / 1065 , % 14.1\n",
      "number:  151 / 1065 , % 14.2\n",
      "number:  152 / 1065 , % 14.3\n",
      "number:  153 / 1065 , % 14.4\n",
      "number:  154 / 1065 , % 14.5\n",
      "number:  155 / 1065 , % 14.6\n",
      "number:  156 / 1065 , % 14.6\n",
      "number:  157 / 1065 , % 14.7\n",
      "number:  158 / 1065 , % 14.8\n",
      "number:  159 / 1065 , % 14.9\n",
      "number:  160 / 1065 , % 15.0\n",
      "number:  161 / 1065 , % 15.1\n",
      "number:  162 / 1065 , % 15.2\n",
      "number:  163 / 1065 , % 15.3\n",
      "number:  164 / 1065 , % 15.4\n",
      "number:  165 / 1065 , % 15.5\n",
      "number:  166 / 1065 , % 15.6\n",
      "number:  167 / 1065 , % 15.7\n",
      "number:  168 / 1065 , % 15.8\n",
      "number:  169 / 1065 , % 15.9\n",
      "number:  170 / 1065 , % 16.0\n",
      "number:  171 / 1065 , % 16.1\n",
      "number:  172 / 1065 , % 16.2\n",
      "number:  173 / 1065 , % 16.2\n",
      "number:  174 / 1065 , % 16.3\n",
      "number:  175 / 1065 , % 16.4\n",
      "number:  176 / 1065 , % 16.5\n",
      "number:  177 / 1065 , % 16.6\n",
      "number:  178 / 1065 , % 16.7\n",
      "number:  179 / 1065 , % 16.8\n",
      "number:  180 / 1065 , % 16.9\n",
      "number:  181 / 1065 , % 17.0\n",
      "number:  182 / 1065 , % 17.1\n",
      "number:  183 / 1065 , % 17.2\n",
      "number:  184 / 1065 , % 17.3\n",
      "number:  185 / 1065 , % 17.4\n",
      "number:  186 / 1065 , % 17.5\n",
      "number:  187 / 1065 , % 17.6\n",
      "number:  188 / 1065 , % 17.7\n",
      "number:  189 / 1065 , % 17.7\n",
      "number:  190 / 1065 , % 17.8\n",
      "number:  191 / 1065 , % 17.9\n",
      "number:  192 / 1065 , % 18.0\n",
      "number:  193 / 1065 , % 18.1\n",
      "number:  194 / 1065 , % 18.2\n",
      "number:  195 / 1065 , % 18.3\n",
      "number:  196 / 1065 , % 18.4\n",
      "number:  197 / 1065 , % 18.5\n",
      "number:  198 / 1065 , % 18.6\n",
      "number:  199 / 1065 , % 18.7\n",
      "number:  200 / 1065 , % 18.8\n",
      "number:  201 / 1065 , % 18.9\n",
      "number:  202 / 1065 , % 19.0\n",
      "number:  203 / 1065 , % 19.1\n",
      "number:  204 / 1065 , % 19.2\n",
      "number:  205 / 1065 , % 19.2\n",
      "number:  206 / 1065 , % 19.3\n",
      "number:  207 / 1065 , % 19.4\n",
      "number:  208 / 1065 , % 19.5\n",
      "number:  209 / 1065 , % 19.6\n",
      "number:  210 / 1065 , % 19.7\n",
      "number:  211 / 1065 , % 19.8\n",
      "number:  212 / 1065 , % 19.9\n",
      "number:  213 / 1065 , % 20.0\n",
      "number:  214 / 1065 , % 20.1\n",
      "number:  215 / 1065 , % 20.2\n",
      "number:  216 / 1065 , % 20.3\n",
      "number:  217 / 1065 , % 20.4\n",
      "number:  218 / 1065 , % 20.5\n",
      "number:  219 / 1065 , % 20.6\n",
      "number:  220 / 1065 , % 20.7\n",
      "number:  221 / 1065 , % 20.8\n",
      "number:  222 / 1065 , % 20.8\n",
      "number:  223 / 1065 , % 20.9\n",
      "number:  224 / 1065 , % 21.0\n",
      "number:  225 / 1065 , % 21.1\n",
      "number:  226 / 1065 , % 21.2\n",
      "number:  227 / 1065 , % 21.3\n",
      "number:  228 / 1065 , % 21.4\n",
      "number:  229 / 1065 , % 21.5\n",
      "number:  230 / 1065 , % 21.6\n",
      "number:  231 / 1065 , % 21.7\n",
      "number:  232 / 1065 , % 21.8\n",
      "number:  233 / 1065 , % 21.9\n",
      "number:  234 / 1065 , % 22.0\n",
      "number:  235 / 1065 , % 22.1\n",
      "number:  236 / 1065 , % 22.2\n",
      "number:  237 / 1065 , % 22.3\n",
      "number:  238 / 1065 , % 22.3\n",
      "number:  239 / 1065 , % 22.4\n",
      "number:  240 / 1065 , % 22.5\n",
      "number:  241 / 1065 , % 22.6\n",
      "number:  242 / 1065 , % 22.7\n",
      "number:  243 / 1065 , % 22.8\n",
      "number:  244 / 1065 , % 22.9\n",
      "number:  245 / 1065 , % 23.0\n",
      "number:  246 / 1065 , % 23.1\n",
      "number:  247 / 1065 , % 23.2\n",
      "number:  248 / 1065 , % 23.3\n",
      "number:  249 / 1065 , % 23.4\n",
      "number:  250 / 1065 , % 23.5\n",
      "number:  251 / 1065 , % 23.6\n",
      "number:  252 / 1065 , % 23.7\n",
      "number:  253 / 1065 , % 23.8\n",
      "number:  254 / 1065 , % 23.8\n",
      "number:  255 / 1065 , % 23.9\n",
      "number:  256 / 1065 , % 24.0\n",
      "number:  257 / 1065 , % 24.1\n",
      "number:  258 / 1065 , % 24.2\n",
      "number:  259 / 1065 , % 24.3\n",
      "number:  260 / 1065 , % 24.4\n",
      "number:  261 / 1065 , % 24.5\n",
      "number:  262 / 1065 , % 24.6\n",
      "number:  263 / 1065 , % 24.7\n",
      "number:  264 / 1065 , % 24.8\n",
      "number:  265 / 1065 , % 24.9\n",
      "number:  266 / 1065 , % 25.0\n",
      "number:  267 / 1065 , % 25.1\n",
      "number:  268 / 1065 , % 25.2\n",
      "number:  269 / 1065 , % 25.3\n",
      "number:  270 / 1065 , % 25.4\n",
      "number:  271 / 1065 , % 25.4\n",
      "number:  272 / 1065 , % 25.5\n",
      "number:  273 / 1065 , % 25.6\n",
      "number:  274 / 1065 , % 25.7\n",
      "number:  275 / 1065 , % 25.8\n",
      "number:  276 / 1065 , % 25.9\n",
      "number:  277 / 1065 , % 26.0\n",
      "number:  278 / 1065 , % 26.1\n",
      "number:  279 / 1065 , % 26.2\n",
      "number:  280 / 1065 , % 26.3\n",
      "number:  281 / 1065 , % 26.4\n",
      "number:  282 / 1065 , % 26.5\n",
      "number:  283 / 1065 , % 26.6\n",
      "number:  284 / 1065 , % 26.7\n",
      "number:  285 / 1065 , % 26.8\n",
      "number:  286 / 1065 , % 26.9\n",
      "number:  287 / 1065 , % 26.9\n",
      "number:  288 / 1065 , % 27.0\n",
      "number:  289 / 1065 , % 27.1\n",
      "number:  290 / 1065 , % 27.2\n",
      "number:  291 / 1065 , % 27.3\n",
      "number:  292 / 1065 , % 27.4\n",
      "number:  293 / 1065 , % 27.5\n",
      "number:  294 / 1065 , % 27.6\n",
      "number:  295 / 1065 , % 27.7\n",
      "number:  296 / 1065 , % 27.8\n",
      "number:  297 / 1065 , % 27.9\n",
      "number:  298 / 1065 , % 28.0\n",
      "number:  299 / 1065 , % 28.1\n",
      "number:  300 / 1065 , % 28.2\n",
      "number:  301 / 1065 , % 28.3\n",
      "number:  302 / 1065 , % 28.4\n",
      "number:  303 / 1065 , % 28.5\n",
      "number:  304 / 1065 , % 28.5\n",
      "number:  305 / 1065 , % 28.6\n",
      "number:  306 / 1065 , % 28.7\n",
      "number:  307 / 1065 , % 28.8\n",
      "number:  308 / 1065 , % 28.9\n",
      "number:  309 / 1065 , % 29.0\n",
      "number:  310 / 1065 , % 29.1\n",
      "number:  311 / 1065 , % 29.2\n",
      "number:  312 / 1065 , % 29.3\n",
      "number:  313 / 1065 , % 29.4\n",
      "number:  314 / 1065 , % 29.5\n",
      "number:  315 / 1065 , % 29.6\n",
      "number:  316 / 1065 , % 29.7\n",
      "number:  317 / 1065 , % 29.8\n",
      "number:  318 / 1065 , % 29.9\n",
      "number:  319 / 1065 , % 30.0\n",
      "number:  320 / 1065 , % 30.0\n",
      "number:  321 / 1065 , % 30.1\n",
      "number:  322 / 1065 , % 30.2\n",
      "number:  323 / 1065 , % 30.3\n",
      "number:  324 / 1065 , % 30.4\n",
      "number:  325 / 1065 , % 30.5\n",
      "number:  326 / 1065 , % 30.6\n",
      "number:  327 / 1065 , % 30.7\n",
      "number:  328 / 1065 , % 30.8\n",
      "number:  329 / 1065 , % 30.9\n",
      "number:  330 / 1065 , % 31.0\n",
      "number:  331 / 1065 , % 31.1\n",
      "number:  332 / 1065 , % 31.2\n",
      "number:  333 / 1065 , % 31.3\n",
      "number:  334 / 1065 , % 31.4\n",
      "number:  335 / 1065 , % 31.5\n",
      "number:  336 / 1065 , % 31.5\n",
      "number:  337 / 1065 , % 31.6\n",
      "number:  338 / 1065 , % 31.7\n",
      "number:  339 / 1065 , % 31.8\n",
      "number:  340 / 1065 , % 31.9\n",
      "number:  341 / 1065 , % 32.0\n",
      "number:  342 / 1065 , % 32.1\n",
      "number:  343 / 1065 , % 32.2\n",
      "number:  344 / 1065 , % 32.3\n",
      "number:  345 / 1065 , % 32.4\n",
      "number:  346 / 1065 , % 32.5\n",
      "number:  347 / 1065 , % 32.6\n",
      "number:  348 / 1065 , % 32.7\n",
      "number:  349 / 1065 , % 32.8\n",
      "number:  350 / 1065 , % 32.9\n",
      "number:  351 / 1065 , % 33.0\n",
      "number:  352 / 1065 , % 33.1\n",
      "number:  353 / 1065 , % 33.1\n",
      "number:  354 / 1065 , % 33.2\n",
      "number:  355 / 1065 , % 33.3\n",
      "number:  356 / 1065 , % 33.4\n",
      "number:  357 / 1065 , % 33.5\n",
      "number:  358 / 1065 , % 33.6\n",
      "number:  359 / 1065 , % 33.7\n",
      "number:  360 / 1065 , % 33.8\n",
      "number:  361 / 1065 , % 33.9\n",
      "number:  362 / 1065 , % 34.0\n",
      "number:  363 / 1065 , % 34.1\n",
      "number:  364 / 1065 , % 34.2\n",
      "number:  365 / 1065 , % 34.3\n",
      "number:  366 / 1065 , % 34.4\n",
      "number:  367 / 1065 , % 34.5\n",
      "number:  368 / 1065 , % 34.6\n",
      "number:  369 / 1065 , % 34.6\n",
      "number:  370 / 1065 , % 34.7\n",
      "number:  371 / 1065 , % 34.8\n",
      "number:  372 / 1065 , % 34.9\n",
      "number:  373 / 1065 , % 35.0\n",
      "number:  374 / 1065 , % 35.1\n",
      "number:  375 / 1065 , % 35.2\n",
      "number:  376 / 1065 , % 35.3\n",
      "number:  377 / 1065 , % 35.4\n",
      "number:  378 / 1065 , % 35.5\n",
      "number:  379 / 1065 , % 35.6\n",
      "number:  380 / 1065 , % 35.7\n",
      "number:  381 / 1065 , % 35.8\n",
      "number:  382 / 1065 , % 35.9\n",
      "number:  383 / 1065 , % 36.0\n",
      "number:  384 / 1065 , % 36.1\n",
      "number:  385 / 1065 , % 36.2\n",
      "number:  386 / 1065 , % 36.2\n",
      "number:  387 / 1065 , % 36.3\n",
      "number:  388 / 1065 , % 36.4\n",
      "number:  389 / 1065 , % 36.5\n",
      "number:  390 / 1065 , % 36.6\n",
      "number:  391 / 1065 , % 36.7\n",
      "number:  392 / 1065 , % 36.8\n",
      "number:  393 / 1065 , % 36.9\n",
      "number:  394 / 1065 , % 37.0\n",
      "number:  395 / 1065 , % 37.1\n",
      "number:  396 / 1065 , % 37.2\n",
      "number:  397 / 1065 , % 37.3\n",
      "number:  398 / 1065 , % 37.4\n",
      "number:  399 / 1065 , % 37.5\n",
      "number:  400 / 1065 , % 37.6\n",
      "number:  401 / 1065 , % 37.7\n",
      "number:  402 / 1065 , % 37.7\n",
      "number:  403 / 1065 , % 37.8\n",
      "number:  404 / 1065 , % 37.9\n",
      "number:  405 / 1065 , % 38.0\n",
      "number:  406 / 1065 , % 38.1\n",
      "number:  407 / 1065 , % 38.2\n",
      "number:  408 / 1065 , % 38.3\n",
      "number:  409 / 1065 , % 38.4\n",
      "number:  410 / 1065 , % 38.5\n",
      "number:  411 / 1065 , % 38.6\n",
      "number:  412 / 1065 , % 38.7\n",
      "number:  413 / 1065 , % 38.8\n",
      "number:  414 / 1065 , % 38.9\n",
      "number:  415 / 1065 , % 39.0\n",
      "number:  416 / 1065 , % 39.1\n",
      "number:  417 / 1065 , % 39.2\n",
      "number:  418 / 1065 , % 39.2\n",
      "number:  419 / 1065 , % 39.3\n",
      "number:  420 / 1065 , % 39.4\n",
      "number:  421 / 1065 , % 39.5\n",
      "number:  422 / 1065 , % 39.6\n",
      "number:  423 / 1065 , % 39.7\n",
      "number:  424 / 1065 , % 39.8\n",
      "number:  425 / 1065 , % 39.9\n",
      "number:  426 / 1065 , % 40.0\n",
      "number:  427 / 1065 , % 40.1\n",
      "number:  428 / 1065 , % 40.2\n",
      "number:  429 / 1065 , % 40.3\n",
      "number:  430 / 1065 , % 40.4\n",
      "number:  431 / 1065 , % 40.5\n",
      "number:  432 / 1065 , % 40.6\n",
      "number:  433 / 1065 , % 40.7\n",
      "number:  434 / 1065 , % 40.8\n",
      "number:  435 / 1065 , % 40.8\n",
      "number:  436 / 1065 , % 40.9\n",
      "number:  437 / 1065 , % 41.0\n",
      "number:  438 / 1065 , % 41.1\n",
      "number:  439 / 1065 , % 41.2\n",
      "number:  440 / 1065 , % 41.3\n",
      "number:  441 / 1065 , % 41.4\n",
      "number:  442 / 1065 , % 41.5\n",
      "number:  443 / 1065 , % 41.6\n",
      "number:  444 / 1065 , % 41.7\n",
      "number:  445 / 1065 , % 41.8\n",
      "number:  446 / 1065 , % 41.9\n",
      "number:  447 / 1065 , % 42.0\n",
      "number:  448 / 1065 , % 42.1\n",
      "number:  449 / 1065 , % 42.2\n",
      "number:  450 / 1065 , % 42.3\n",
      "number:  451 / 1065 , % 42.3\n",
      "number:  452 / 1065 , % 42.4\n",
      "number:  453 / 1065 , % 42.5\n",
      "number:  454 / 1065 , % 42.6\n",
      "number:  455 / 1065 , % 42.7\n",
      "number:  456 / 1065 , % 42.8\n",
      "number:  457 / 1065 , % 42.9\n",
      "number:  458 / 1065 , % 43.0\n",
      "number:  459 / 1065 , % 43.1\n",
      "number:  460 / 1065 , % 43.2\n",
      "number:  461 / 1065 , % 43.3\n",
      "number:  462 / 1065 , % 43.4\n",
      "number:  463 / 1065 , % 43.5\n",
      "number:  464 / 1065 , % 43.6\n",
      "number:  465 / 1065 , % 43.7\n",
      "number:  466 / 1065 , % 43.8\n",
      "number:  467 / 1065 , % 43.8\n",
      "number:  468 / 1065 , % 43.9\n",
      "number:  469 / 1065 , % 44.0\n",
      "number:  470 / 1065 , % 44.1\n",
      "number:  471 / 1065 , % 44.2\n",
      "number:  472 / 1065 , % 44.3\n",
      "number:  473 / 1065 , % 44.4\n",
      "number:  474 / 1065 , % 44.5\n",
      "number:  475 / 1065 , % 44.6\n",
      "number:  476 / 1065 , % 44.7\n",
      "number:  477 / 1065 , % 44.8\n",
      "number:  478 / 1065 , % 44.9\n",
      "number:  479 / 1065 , % 45.0\n",
      "number:  480 / 1065 , % 45.1\n",
      "number:  481 / 1065 , % 45.2\n",
      "number:  482 / 1065 , % 45.3\n",
      "number:  483 / 1065 , % 45.4\n",
      "number:  484 / 1065 , % 45.4\n",
      "number:  485 / 1065 , % 45.5\n",
      "number:  486 / 1065 , % 45.6\n",
      "number:  487 / 1065 , % 45.7\n",
      "number:  488 / 1065 , % 45.8\n",
      "number:  489 / 1065 , % 45.9\n",
      "number:  490 / 1065 , % 46.0\n",
      "number:  491 / 1065 , % 46.1\n",
      "number:  492 / 1065 , % 46.2\n",
      "number:  493 / 1065 , % 46.3\n",
      "number:  494 / 1065 , % 46.4\n",
      "number:  495 / 1065 , % 46.5\n",
      "number:  496 / 1065 , % 46.6\n",
      "number:  497 / 1065 , % 46.7\n",
      "number:  498 / 1065 , % 46.8\n",
      "number:  499 / 1065 , % 46.9\n",
      "number:  500 / 1065 , % 46.9\n",
      "number:  501 / 1065 , % 47.0\n",
      "number:  502 / 1065 , % 47.1\n",
      "number:  503 / 1065 , % 47.2\n",
      "number:  504 / 1065 , % 47.3\n",
      "number:  505 / 1065 , % 47.4\n",
      "number:  506 / 1065 , % 47.5\n",
      "number:  507 / 1065 , % 47.6\n",
      "number:  508 / 1065 , % 47.7\n",
      "number:  509 / 1065 , % 47.8\n",
      "number:  510 / 1065 , % 47.9\n",
      "number:  511 / 1065 , % 48.0\n",
      "number:  512 / 1065 , % 48.1\n",
      "number:  513 / 1065 , % 48.2\n",
      "number:  514 / 1065 , % 48.3\n",
      "number:  515 / 1065 , % 48.4\n",
      "number:  516 / 1065 , % 48.5\n",
      "number:  517 / 1065 , % 48.5\n",
      "number:  518 / 1065 , % 48.6\n",
      "number:  519 / 1065 , % 48.7\n",
      "number:  520 / 1065 , % 48.8\n",
      "number:  521 / 1065 , % 48.9\n",
      "number:  522 / 1065 , % 49.0\n",
      "number:  523 / 1065 , % 49.1\n",
      "number:  524 / 1065 , % 49.2\n",
      "number:  525 / 1065 , % 49.3\n",
      "number:  526 / 1065 , % 49.4\n",
      "number:  527 / 1065 , % 49.5\n",
      "number:  528 / 1065 , % 49.6\n",
      "number:  529 / 1065 , % 49.7\n",
      "number:  530 / 1065 , % 49.8\n",
      "number:  531 / 1065 , % 49.9\n",
      "number:  532 / 1065 , % 50.0\n",
      "number:  533 / 1065 , % 50.0\n",
      "number:  534 / 1065 , % 50.1\n",
      "number:  535 / 1065 , % 50.2\n",
      "number:  536 / 1065 , % 50.3\n",
      "number:  537 / 1065 , % 50.4\n",
      "number:  538 / 1065 , % 50.5\n",
      "number:  539 / 1065 , % 50.6\n",
      "number:  540 / 1065 , % 50.7\n",
      "number:  541 / 1065 , % 50.8\n",
      "number:  542 / 1065 , % 50.9\n",
      "number:  543 / 1065 , % 51.0\n",
      "number:  544 / 1065 , % 51.1\n",
      "number:  545 / 1065 , % 51.2\n",
      "number:  546 / 1065 , % 51.3\n",
      "number:  547 / 1065 , % 51.4\n",
      "number:  548 / 1065 , % 51.5\n",
      "number:  549 / 1065 , % 51.5\n",
      "number:  550 / 1065 , % 51.6\n",
      "number:  551 / 1065 , % 51.7\n",
      "number:  552 / 1065 , % 51.8\n",
      "number:  553 / 1065 , % 51.9\n",
      "number:  554 / 1065 , % 52.0\n",
      "number:  555 / 1065 , % 52.1\n",
      "number:  556 / 1065 , % 52.2\n",
      "number:  557 / 1065 , % 52.3\n",
      "number:  558 / 1065 , % 52.4\n",
      "number:  559 / 1065 , % 52.5\n",
      "number:  560 / 1065 , % 52.6\n",
      "number:  561 / 1065 , % 52.7\n",
      "number:  562 / 1065 , % 52.8\n",
      "number:  563 / 1065 , % 52.9\n",
      "number:  564 / 1065 , % 53.0\n",
      "number:  565 / 1065 , % 53.1\n",
      "number:  566 / 1065 , % 53.1\n",
      "number:  567 / 1065 , % 53.2\n",
      "number:  568 / 1065 , % 53.3\n",
      "number:  569 / 1065 , % 53.4\n",
      "number:  570 / 1065 , % 53.5\n",
      "number:  571 / 1065 , % 53.6\n",
      "number:  572 / 1065 , % 53.7\n",
      "number:  573 / 1065 , % 53.8\n",
      "number:  574 / 1065 , % 53.9\n",
      "number:  575 / 1065 , % 54.0\n",
      "number:  576 / 1065 , % 54.1\n",
      "number:  577 / 1065 , % 54.2\n",
      "number:  578 / 1065 , % 54.3\n",
      "number:  579 / 1065 , % 54.4\n",
      "number:  580 / 1065 , % 54.5\n",
      "number:  581 / 1065 , % 54.6\n",
      "number:  582 / 1065 , % 54.6\n",
      "number:  583 / 1065 , % 54.7\n",
      "number:  584 / 1065 , % 54.8\n",
      "number:  585 / 1065 , % 54.9\n",
      "number:  586 / 1065 , % 55.0\n",
      "number:  587 / 1065 , % 55.1\n",
      "number:  588 / 1065 , % 55.2\n",
      "number:  589 / 1065 , % 55.3\n",
      "number:  590 / 1065 , % 55.4\n",
      "number:  591 / 1065 , % 55.5\n",
      "number:  592 / 1065 , % 55.6\n",
      "number:  593 / 1065 , % 55.7\n",
      "number:  594 / 1065 , % 55.8\n",
      "number:  595 / 1065 , % 55.9\n",
      "number:  596 / 1065 , % 56.0\n",
      "number:  597 / 1065 , % 56.1\n",
      "number:  598 / 1065 , % 56.2\n",
      "number:  599 / 1065 , % 56.2\n",
      "number:  600 / 1065 , % 56.3\n",
      "number:  601 / 1065 , % 56.4\n",
      "number:  602 / 1065 , % 56.5\n",
      "number:  603 / 1065 , % 56.6\n",
      "number:  604 / 1065 , % 56.7\n",
      "number:  605 / 1065 , % 56.8\n",
      "number:  606 / 1065 , % 56.9\n",
      "number:  607 / 1065 , % 57.0\n",
      "number:  608 / 1065 , % 57.1\n",
      "number:  609 / 1065 , % 57.2\n",
      "number:  610 / 1065 , % 57.3\n",
      "number:  611 / 1065 , % 57.4\n",
      "number:  612 / 1065 , % 57.5\n",
      "number:  613 / 1065 , % 57.6\n",
      "number:  614 / 1065 , % 57.7\n",
      "number:  615 / 1065 , % 57.7\n",
      "number:  616 / 1065 , % 57.8\n",
      "number:  617 / 1065 , % 57.9\n",
      "number:  618 / 1065 , % 58.0\n",
      "number:  619 / 1065 , % 58.1\n",
      "number:  620 / 1065 , % 58.2\n",
      "number:  621 / 1065 , % 58.3\n",
      "number:  622 / 1065 , % 58.4\n",
      "number:  623 / 1065 , % 58.5\n",
      "number:  624 / 1065 , % 58.6\n",
      "number:  625 / 1065 , % 58.7\n",
      "number:  626 / 1065 , % 58.8\n",
      "number:  627 / 1065 , % 58.9\n",
      "number:  628 / 1065 , % 59.0\n",
      "number:  629 / 1065 , % 59.1\n",
      "number:  630 / 1065 , % 59.2\n",
      "number:  631 / 1065 , % 59.2\n",
      "number:  632 / 1065 , % 59.3\n",
      "number:  633 / 1065 , % 59.4\n",
      "number:  634 / 1065 , % 59.5\n",
      "number:  635 / 1065 , % 59.6\n",
      "number:  636 / 1065 , % 59.7\n",
      "number:  637 / 1065 , % 59.8\n",
      "number:  638 / 1065 , % 59.9\n",
      "number:  639 / 1065 , % 60.0\n",
      "number:  640 / 1065 , % 60.1\n",
      "number:  641 / 1065 , % 60.2\n",
      "number:  642 / 1065 , % 60.3\n",
      "number:  643 / 1065 , % 60.4\n",
      "number:  644 / 1065 , % 60.5\n",
      "number:  645 / 1065 , % 60.6\n",
      "number:  646 / 1065 , % 60.7\n",
      "number:  647 / 1065 , % 60.8\n",
      "number:  648 / 1065 , % 60.8\n",
      "number:  649 / 1065 , % 60.9\n",
      "number:  650 / 1065 , % 61.0\n",
      "number:  651 / 1065 , % 61.1\n",
      "number:  652 / 1065 , % 61.2\n",
      "number:  653 / 1065 , % 61.3\n",
      "number:  654 / 1065 , % 61.4\n",
      "number:  655 / 1065 , % 61.5\n",
      "number:  656 / 1065 , % 61.6\n",
      "number:  657 / 1065 , % 61.7\n",
      "number:  658 / 1065 , % 61.8\n",
      "number:  659 / 1065 , % 61.9\n",
      "number:  660 / 1065 , % 62.0\n",
      "number:  661 / 1065 , % 62.1\n",
      "number:  662 / 1065 , % 62.2\n",
      "number:  663 / 1065 , % 62.3\n",
      "number:  664 / 1065 , % 62.3\n",
      "number:  665 / 1065 , % 62.4\n",
      "number:  666 / 1065 , % 62.5\n",
      "number:  667 / 1065 , % 62.6\n",
      "number:  668 / 1065 , % 62.7\n",
      "number:  669 / 1065 , % 62.8\n",
      "number:  670 / 1065 , % 62.9\n",
      "number:  671 / 1065 , % 63.0\n",
      "number:  672 / 1065 , % 63.1\n",
      "number:  673 / 1065 , % 63.2\n",
      "number:  674 / 1065 , % 63.3\n",
      "number:  675 / 1065 , % 63.4\n",
      "number:  676 / 1065 , % 63.5\n",
      "number:  677 / 1065 , % 63.6\n",
      "number:  678 / 1065 , % 63.7\n",
      "number:  679 / 1065 , % 63.8\n",
      "number:  680 / 1065 , % 63.8\n",
      "number:  681 / 1065 , % 63.9\n",
      "number:  682 / 1065 , % 64.0\n",
      "number:  683 / 1065 , % 64.1\n",
      "number:  684 / 1065 , % 64.2\n",
      "number:  685 / 1065 , % 64.3\n",
      "number:  686 / 1065 , % 64.4\n",
      "number:  687 / 1065 , % 64.5\n",
      "number:  688 / 1065 , % 64.6\n",
      "number:  689 / 1065 , % 64.7\n",
      "number:  690 / 1065 , % 64.8\n",
      "number:  691 / 1065 , % 64.9\n",
      "number:  692 / 1065 , % 65.0\n",
      "number:  693 / 1065 , % 65.1\n",
      "number:  694 / 1065 , % 65.2\n",
      "number:  695 / 1065 , % 65.3\n",
      "number:  696 / 1065 , % 65.4\n",
      "number:  697 / 1065 , % 65.4\n",
      "number:  698 / 1065 , % 65.5\n",
      "number:  699 / 1065 , % 65.6\n",
      "number:  700 / 1065 , % 65.7\n",
      "number:  701 / 1065 , % 65.8\n",
      "number:  702 / 1065 , % 65.9\n",
      "number:  703 / 1065 , % 66.0\n",
      "number:  704 / 1065 , % 66.1\n",
      "number:  705 / 1065 , % 66.2\n",
      "number:  706 / 1065 , % 66.3\n",
      "number:  707 / 1065 , % 66.4\n",
      "number:  708 / 1065 , % 66.5\n",
      "number:  709 / 1065 , % 66.6\n",
      "number:  710 / 1065 , % 66.7\n",
      "number:  711 / 1065 , % 66.8\n",
      "number:  712 / 1065 , % 66.9\n",
      "number:  713 / 1065 , % 66.9\n",
      "number:  714 / 1065 , % 67.0\n",
      "number:  715 / 1065 , % 67.1\n",
      "number:  716 / 1065 , % 67.2\n",
      "number:  717 / 1065 , % 67.3\n",
      "number:  718 / 1065 , % 67.4\n",
      "number:  719 / 1065 , % 67.5\n",
      "number:  720 / 1065 , % 67.6\n",
      "number:  721 / 1065 , % 67.7\n",
      "number:  722 / 1065 , % 67.8\n",
      "number:  723 / 1065 , % 67.9\n",
      "number:  724 / 1065 , % 68.0\n",
      "number:  725 / 1065 , % 68.1\n",
      "number:  726 / 1065 , % 68.2\n",
      "number:  727 / 1065 , % 68.3\n",
      "number:  728 / 1065 , % 68.4\n",
      "number:  729 / 1065 , % 68.5\n",
      "number:  730 / 1065 , % 68.5\n",
      "number:  731 / 1065 , % 68.6\n",
      "number:  732 / 1065 , % 68.7\n",
      "number:  733 / 1065 , % 68.8\n",
      "number:  734 / 1065 , % 68.9\n",
      "number:  735 / 1065 , % 69.0\n",
      "number:  736 / 1065 , % 69.1\n",
      "number:  737 / 1065 , % 69.2\n",
      "number:  738 / 1065 , % 69.3\n",
      "number:  739 / 1065 , % 69.4\n",
      "number:  740 / 1065 , % 69.5\n",
      "number:  741 / 1065 , % 69.6\n",
      "number:  742 / 1065 , % 69.7\n",
      "number:  743 / 1065 , % 69.8\n",
      "number:  744 / 1065 , % 69.9\n",
      "number:  745 / 1065 , % 70.0\n",
      "number:  746 / 1065 , % 70.0\n",
      "number:  747 / 1065 , % 70.1\n",
      "number:  748 / 1065 , % 70.2\n",
      "number:  749 / 1065 , % 70.3\n",
      "number:  750 / 1065 , % 70.4\n",
      "number:  751 / 1065 , % 70.5\n",
      "number:  752 / 1065 , % 70.6\n",
      "number:  753 / 1065 , % 70.7\n",
      "number:  754 / 1065 , % 70.8\n",
      "number:  755 / 1065 , % 70.9\n",
      "number:  756 / 1065 , % 71.0\n",
      "number:  757 / 1065 , % 71.1\n",
      "number:  758 / 1065 , % 71.2\n",
      "number:  759 / 1065 , % 71.3\n",
      "number:  760 / 1065 , % 71.4\n",
      "number:  761 / 1065 , % 71.5\n",
      "number:  762 / 1065 , % 71.5\n",
      "number:  763 / 1065 , % 71.6\n",
      "number:  764 / 1065 , % 71.7\n",
      "number:  765 / 1065 , % 71.8\n",
      "number:  766 / 1065 , % 71.9\n",
      "number:  767 / 1065 , % 72.0\n",
      "number:  768 / 1065 , % 72.1\n",
      "number:  769 / 1065 , % 72.2\n",
      "number:  770 / 1065 , % 72.3\n",
      "number:  771 / 1065 , % 72.4\n",
      "number:  772 / 1065 , % 72.5\n",
      "number:  773 / 1065 , % 72.6\n",
      "number:  774 / 1065 , % 72.7\n",
      "number:  775 / 1065 , % 72.8\n",
      "number:  776 / 1065 , % 72.9\n",
      "number:  777 / 1065 , % 73.0\n",
      "number:  778 / 1065 , % 73.1\n",
      "number:  779 / 1065 , % 73.1\n",
      "number:  780 / 1065 , % 73.2\n",
      "number:  781 / 1065 , % 73.3\n",
      "number:  782 / 1065 , % 73.4\n",
      "number:  783 / 1065 , % 73.5\n",
      "number:  784 / 1065 , % 73.6\n",
      "number:  785 / 1065 , % 73.7\n",
      "number:  786 / 1065 , % 73.8\n",
      "number:  787 / 1065 , % 73.9\n",
      "number:  788 / 1065 , % 74.0\n",
      "number:  789 / 1065 , % 74.1\n",
      "number:  790 / 1065 , % 74.2\n",
      "number:  791 / 1065 , % 74.3\n",
      "number:  792 / 1065 , % 74.4\n",
      "number:  793 / 1065 , % 74.5\n",
      "number:  794 / 1065 , % 74.6\n",
      "number:  795 / 1065 , % 74.6\n",
      "number:  796 / 1065 , % 74.7\n",
      "number:  797 / 1065 , % 74.8\n",
      "number:  798 / 1065 , % 74.9\n",
      "number:  799 / 1065 , % 75.0\n",
      "number:  800 / 1065 , % 75.1\n",
      "number:  801 / 1065 , % 75.2\n",
      "number:  802 / 1065 , % 75.3\n",
      "number:  803 / 1065 , % 75.4\n",
      "number:  804 / 1065 , % 75.5\n",
      "number:  805 / 1065 , % 75.6\n",
      "number:  806 / 1065 , % 75.7\n",
      "number:  807 / 1065 , % 75.8\n",
      "number:  808 / 1065 , % 75.9\n",
      "number:  809 / 1065 , % 76.0\n",
      "number:  810 / 1065 , % 76.1\n",
      "number:  811 / 1065 , % 76.2\n",
      "number:  812 / 1065 , % 76.2\n",
      "number:  813 / 1065 , % 76.3\n",
      "number:  814 / 1065 , % 76.4\n",
      "number:  815 / 1065 , % 76.5\n",
      "number:  816 / 1065 , % 76.6\n",
      "number:  817 / 1065 , % 76.7\n",
      "number:  818 / 1065 , % 76.8\n",
      "number:  819 / 1065 , % 76.9\n",
      "number:  820 / 1065 , % 77.0\n",
      "number:  821 / 1065 , % 77.1\n",
      "number:  822 / 1065 , % 77.2\n",
      "number:  823 / 1065 , % 77.3\n",
      "number:  824 / 1065 , % 77.4\n",
      "number:  825 / 1065 , % 77.5\n",
      "number:  826 / 1065 , % 77.6\n",
      "number:  827 / 1065 , % 77.7\n",
      "number:  828 / 1065 , % 77.7\n",
      "number:  829 / 1065 , % 77.8\n",
      "number:  830 / 1065 , % 77.9\n",
      "number:  831 / 1065 , % 78.0\n",
      "number:  832 / 1065 , % 78.1\n",
      "number:  833 / 1065 , % 78.2\n",
      "number:  834 / 1065 , % 78.3\n",
      "number:  835 / 1065 , % 78.4\n",
      "number:  836 / 1065 , % 78.5\n",
      "number:  837 / 1065 , % 78.6\n",
      "number:  838 / 1065 , % 78.7\n",
      "number:  839 / 1065 , % 78.8\n",
      "number:  840 / 1065 , % 78.9\n",
      "number:  841 / 1065 , % 79.0\n",
      "number:  842 / 1065 , % 79.1\n",
      "number:  843 / 1065 , % 79.2\n",
      "number:  844 / 1065 , % 79.2\n",
      "number:  845 / 1065 , % 79.3\n",
      "number:  846 / 1065 , % 79.4\n",
      "number:  847 / 1065 , % 79.5\n",
      "number:  848 / 1065 , % 79.6\n",
      "number:  849 / 1065 , % 79.7\n",
      "number:  850 / 1065 , % 79.8\n",
      "number:  851 / 1065 , % 79.9\n",
      "number:  852 / 1065 , % 80.0\n",
      "number:  853 / 1065 , % 80.1\n",
      "number:  854 / 1065 , % 80.2\n",
      "number:  855 / 1065 , % 80.3\n",
      "number:  856 / 1065 , % 80.4\n",
      "number:  857 / 1065 , % 80.5\n",
      "number:  858 / 1065 , % 80.6\n",
      "number:  859 / 1065 , % 80.7\n",
      "number:  860 / 1065 , % 80.8\n",
      "number:  861 / 1065 , % 80.8\n",
      "number:  862 / 1065 , % 80.9\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "errorX = []\n",
    "errorY = []\n",
    "errorZ = []\n",
    "errorMX = 0\n",
    "errorMY = 0\n",
    "errorMZ = 0\n",
    "\n",
    "rows, colms = test_cp.shape\n",
    "print('Number of tests: ', rows )\n",
    "\n",
    "y = 0\n",
    "for x in test_cp:\n",
    "  p = model.predict(x, verbose=0)\n",
    "  answer = test_ans[y][0], test_ans[y][1], test_ans[y][2]\n",
    "  \n",
    "  eX = answer[0] - p[0][0]\n",
    "  eY = answer[1] - p[0][1]\n",
    "  eZ = answer[2] - p[0][2]\n",
    "  \n",
    "  errorX.append(eX)\n",
    "  errorY.append(eY)\n",
    "  errorZ.append(eZ) \n",
    "  \n",
    "  if(errorMX < abs(eX)):\n",
    "    errorMX = abs(eX)\n",
    "  if(errorMY < abs(eY)):\n",
    "    errorMY = abs(eY)\n",
    "  if(errorMZ < abs(eZ)):\n",
    "    errorMZ = abs(eZ)\n",
    "\n",
    "  print('number: ', y, '/', rows, ', %',round(((y/rows)*100), 1),)\n",
    "  y = y + 1\n",
    "\n",
    "\n",
    "\n",
    "print('Number of tests: ', rows )\n",
    "print('Max error X', round(errorMX, 2), '   average error X: ', round((sum(errorX)/len(errorX)), 2))\n",
    "print('Max error Y', round(errorMY, 2), '   average error Y: ', round((sum(errorY)/len(errorY)), 2))\n",
    "print('Max error Z', round(errorMZ, 2), '   average error Z: ', round((sum(errorZ)/len(errorZ)), 2))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99313c5ed734f8f56544f8fadb47f4b194e4001a240c5eaa2e8e8b97b694ec5b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
